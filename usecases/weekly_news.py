# –¢–µ—Å—Ç–∏—Ä–æ–≤–∫–∞ —Ñ—É–Ω–∫–∏–æ–Ω–∞–ª–∞ –∞–Ω–∞–ª–∏–∑–∞
# model_embed = OpenAI API - text-embedding-3-small | model_LLM = DeepSeek

import sys
import os
sys.path.append(os.path.abspath(os.path.dirname(__file__)))

from typing import Dict, Any, List
from llm_client import get_llm_client
from vector_store import VectorStore
from text_processor import TextProcessor
from logger_config import setup_logger
import tiktoken
from datetime import datetime, timedelta

# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–≥–µ—Ä
logger = setup_logger("test_analysis")

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
ANALYSIS_START_DATE = "2025-06-03"  # –ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞ –Ω–µ–¥–µ–ª–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ YYYY-MM-DD
ANALYSIS_CATEGORY = "–í–∏–¥–µ–æ–∏–≥—Ä—ã"  # –ö–∞—Ç–µ–≥–æ—Ä–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞

def count_tokens(text: str, model: str = "gpt-3.5-turbo") -> int:
    """
    –ü–æ–¥—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ —Å —É—á–µ—Ç–æ–º –º–æ–¥–µ–ª–∏
    
    Args:
        text: –¢–µ–∫—Å—Ç –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤
        model: –ú–æ–¥–µ–ª—å –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        
    Returns:
        int: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
    """
    try:
        encoding = tiktoken.encoding_for_model(model)
        return len(encoding.encode(text))
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥—Å—á–µ—Ç–µ —Ç–æ–∫–µ–Ω–æ–≤: {str(e)}")
        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å tiktoken, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç
        return len(text.split()) * 1.3  # –ü—Ä–∏–º–µ—Ä–Ω—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–ª—è —Å–ª–æ–≤

def calculate_chunk_size(materials: List[Dict[str, Any]], max_context_size: int) -> int:
    """
    –†–∞—Å—á–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —á–∞–Ω–∫–∞ —Å —É—á–µ—Ç–æ–º –ø—Ä–æ–º–ø—Ç–æ–≤ –∏ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
    
    Args:
        materials: –°–ø–∏—Å–æ–∫ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤
        max_context_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –º–æ–¥–µ–ª–∏
        
    Returns:
        int: –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
    """
    # –û—Å—Ç–∞–≤–ª—è–µ–º 30% –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø—Ä–æ–º–ø—Ç–æ–≤ –∏ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π (—É–≤–µ–ª–∏—á–∏–ª–∏ —Å 20% –¥–æ 30%)
    available_tokens = int(max_context_size * 0.7)
    
    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä –º–∞—Ç–µ—Ä–∏–∞–ª–∞
    total_tokens = sum(count_tokens(material['text']) for material in materials)
    avg_tokens_per_material = total_tokens / len(materials)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–ø–∞—Å –≤ 20% –∫ —Å—Ä–µ–¥–Ω–µ–º—É —Ä–∞–∑–º–µ—Ä—É –¥–ª—è —É—á–µ—Ç–∞ –≤–∞—Ä–∏–∞—Ü–∏–∏
    safe_avg_tokens = avg_tokens_per_material * 1.2
    
    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–º–µ—Å—Ç—è—Ç—Å—è –≤ –¥–æ—Å—Ç—É–ø–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ
    return max(1, int(available_tokens / safe_avg_tokens))

def _create_context_aware_chunks(materials: List[Dict[str, Any]], max_context_size: int) -> List[List[Dict[str, Any]]]:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞ –º–æ–¥–µ–ª–∏
    
    Args:
        materials: –°–ø–∏—Å–æ–∫ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤
        max_context_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –º–æ–¥–µ–ª–∏
        
    Returns:
        List[List[Dict[str, Any]]]: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤
    """
    chunks = []
    current_chunk = []
    current_size = 0
    
    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
    chunk_size = calculate_chunk_size(materials, max_context_size)
    logger.info(f"–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: {chunk_size} –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤")
    
    for material in materials:
        material_tokens = count_tokens(material['text'])
        
        if current_size + material_tokens > max_context_size * 0.8:  # –û—Å—Ç–∞–≤–ª—è–µ–º 20% –¥–ª—è –ø—Ä–æ–º–ø—Ç–æ–≤
            if current_chunk:
                chunks.append(current_chunk)
            current_chunk = [material]
            current_size = material_tokens
        else:
            current_chunk.append(material)
            current_size += material_tokens
            
        # –ï—Å–ª–∏ –¥–æ—Å—Ç–∏–≥–ª–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —á–∞–Ω–∫–∞, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π
        if len(current_chunk) >= chunk_size:
            chunks.append(current_chunk)
            current_chunk = []
            current_size = 0
    
    if current_chunk:
        chunks.append(current_chunk)
    
    return chunks

def test_embeddings():
    """
    –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    """
    try:
        text_processor = TextProcessor(
            embedding_type="openai",
            openai_model="text-embedding-3-small"
        )
        test_text = "This is a test sentence for embedding dimension check"
        embedding = text_processor.create_embeddings([test_text])[0]
        logger.info(f"–¢–µ—Å—Ç–æ–≤–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {len(embedding)}")
        logger.info(f"–ü–µ—Ä–≤—ã–µ 5 –∑–Ω–∞—á–µ–Ω–∏–π: {embedding[:5]}")
        return len(embedding)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {str(e)}")
        return None

def analyze_trend(
    category: str,
    analysis_start_date: str,
    embedding_type: str = "openai",
    openai_model: str = "text-embedding-3-small"
) -> Dict[str, Any]:
    """
    –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ –∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–≤–æ–¥–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ –Ω–µ–¥–µ–ª—é
    
    Args:
        category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        analysis_start_date: –ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞ –Ω–µ–¥–µ–ª–∏ (—Å—Ç—Ä–æ–∫–∞)
        embedding_type: –¢–∏–ø —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ ("ollama" –∏–ª–∏ "openai")
        openai_model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è OpenAI
        
    Returns:
        Dict[str, Any]: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
    """
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        llm_client = get_llm_client()
        vector_store = VectorStore(
            embedding_type=embedding_type,
            openai_model=openai_model
        )
        text_processor = TextProcessor(
            embedding_type=embedding_type,
            openai_model=openai_model
        )
        
        # 1. –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –∑–∞ –Ω–µ–¥–µ–ª—é
        logger.info(f"–ü–æ–ª—É—á–∞–µ–º –º–∞—Ç–µ—Ä–∏–∞–ª—ã –∑–∞ –Ω–µ–¥–µ–ª—é –Ω–∞—á–∏–Ω–∞—è —Å {analysis_start_date} –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {category}")
        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ç—Ä–æ–∫—É –¥–∞—Ç—ã –≤ datetime
            start_date = datetime.strptime(analysis_start_date, "%Y-%m-%d")
            end_date = start_date + timedelta(days=6)
            
            # –ü–æ–ª—É—á–∞–µ–º –º–∞—Ç–µ—Ä–∏–∞–ª—ã –∑–∞ –¥–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç
            recent_materials = vector_store.search_by_category_and_date_range(
                category=category,
                start_date=start_date,
                end_date=end_date
            )
            
            if not recent_materials:
                logger.warning(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –∑–∞ –ø–µ—Ä–∏–æ–¥ {analysis_start_date} - {end_date.strftime('%Y-%m-%d')}")
                return {
                    'status': 'error',
                    'message': f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –∑–∞ –ø–µ—Ä–∏–æ–¥ {analysis_start_date} - {end_date.strftime('%Y-%m-%d')}"
                }
                
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(recent_materials)} –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –∑–∞ –ø–µ—Ä–∏–æ–¥ {analysis_start_date} - {end_date.strftime('%Y-%m-%d')}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤: {str(e)}")
            raise

        # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        total_tokens = sum(count_tokens(material['text']) for material in recent_materials)
        max_context_size = llm_client.get_max_context_size()
        logger.info(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤: {total_tokens}")
        logger.info(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –º–æ–¥–µ–ª–∏: {max_context_size}")
        
        # 3. –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏ –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º
        if total_tokens <= max_context_size * 0.8:
            # –ï—Å–ª–∏ –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –Ω–µ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ
            logger.info("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞, –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Å–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã")
            
            # –ü–µ—Ä–≤—ã–π —ç—Ç–∞–ø - –≤—ã–¥–µ–ª–µ–Ω–∏–µ –≥–ª–∞–≤–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
            main_news_prompt = f"""
            –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å—É—Ç–∫–∏ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {category}:
            
            {[material['text'] for material in recent_materials]}
            {[material['url'] for material in recent_materials]}

            –í—ã–¥–µ–ª–∏ –≤—Å–µ —Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏, –∫–æ—Ç–æ—Ä—ã–µ:
            1. –ò–º–µ—é—Ç –Ω–∞–∏–±–æ–ª—å—à–µ–µ –≤–ª–∏—è–Ω–∏–µ –Ω–∞ –∏–Ω–¥—É—Å—Ç—Ä–∏—é
            2. –í—ã–∑–≤–∞–ª–∏ –Ω–∞–∏–±–æ–ª—å—à–∏–π —Ä–µ–∑–æ–Ω–∞–Ω—Å –≤ —Å–æ–æ–±—â–µ—Å—Ç–≤–µ
            3. –ú–æ–≥—É—Ç –ø–æ–≤–ª–∏—è—Ç—å –Ω–∞ –±—É–¥—É—â–∏–µ —Ç—Ä–µ–Ω–¥—ã

            –î–ª—è –∫–∞–∂–¥–æ–π –Ω–æ–≤–æ—Å—Ç–∏ —É–∫–∞–∂–∏:
            - –ó–∞–≥–æ–ª–æ–≤–æ–∫
            - –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
            - –í–ª–∏—è–Ω–∏–µ –Ω–∞ –∏–Ω–¥—É—Å—Ç—Ä–∏—é
            - –†–µ–∞–∫—Ü–∏—é —Å–æ–æ–±—â–µ—Å—Ç–≤–∞
            - –°—Å—ã–ª–∫—É –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫

            –í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ –≤—ã–¥–µ–ª–µ–Ω–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –≤–∏–¥–µ.
            """
            
            main_news_analysis = llm_client.analyze_text(
                prompt=main_news_prompt,
                query="\n".join([material['text'] for material in recent_materials])
            )
            chunk_analyses = [main_news_analysis.get('analysis', '')]
            
        else:
            # –ï—Å–ª–∏ –ø—Ä–µ–≤—ã—à–∞–µ—Ç, –¥–µ–ª–∏–º –º–∞—Ç–µ—Ä–∏–∞–ª—ã –Ω–∞ —á–∞–Ω–∫–∏
            logger.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ, —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏")
            chunks = _create_context_aware_chunks(recent_materials, max_context_size)
            logger.info(f"–ú–∞—Ç–µ—Ä–∏–∞–ª—ã —Ä–∞–∑–±–∏—Ç—ã –Ω–∞ {len(chunks)} —á–∞–Ω–∫–æ–≤")
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —á–∞–Ω–∫
            chunk_analyses = []
            for i, chunk in enumerate(chunks):
                logger.info(f"–ê–Ω–∞–ª–∏–∑ —á–∞–Ω–∫–∞ {i+1}/{len(chunks)}")
                chunk_prompt = f"""
                –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å—É—Ç–∫–∏ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {category}:
                
                {[material['text'] for material in chunk]}
                {[material['url'] for material in chunk]}

                –í—ã–¥–µ–ª–∏ —Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ —ç—Ç–æ–≥–æ —á–∞–Ω–∫–∞, –∫–æ—Ç–æ—Ä—ã–µ:
                1. –ò–º–µ—é—Ç –Ω–∞–∏–±–æ–ª—å—à–µ–µ –≤–ª–∏—è–Ω–∏–µ –Ω–∞ –∏–Ω–¥—É—Å—Ç—Ä–∏—é
                2. –í—ã–∑–≤–∞–ª–∏ –Ω–∞–∏–±–æ–ª—å—à–∏–π —Ä–µ–∑–æ–Ω–∞–Ω—Å –≤ —Å–æ–æ–±—â–µ—Å—Ç–≤–µ
                3. –ú–æ–≥—É—Ç –ø–æ–≤–ª–∏—è—Ç—å –Ω–∞ –±—É–¥—É—â–∏–µ —Ç—Ä–µ–Ω–¥—ã

                –î–ª—è –∫–∞–∂–¥–æ–π –Ω–æ–≤–æ—Å—Ç–∏ —É–∫–∞–∂–∏:
                - –ó–∞–≥–æ–ª–æ–≤–æ–∫
                - –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
                - –í–ª–∏—è–Ω–∏–µ –Ω–∞ –∏–Ω–¥—É—Å—Ç—Ä–∏—é
                - –†–µ–∞–∫—Ü–∏—é —Å–æ–æ–±—â–µ—Å—Ç–≤–∞
                - –°—Å—ã–ª–∫—É –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫

                –í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ –≤—ã–¥–µ–ª–µ–Ω–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –≤–∏–¥–µ.
                """
                
                chunk_analysis = llm_client.analyze_text(
                    prompt=chunk_prompt,
                    query="\n".join([material['text'] for material in chunk])
                )
                chunk_analyses.append(chunk_analysis.get('analysis', ''))
        
        # 4. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å–≤–æ–¥–∫—É –Ω–æ–≤–æ—Å—Ç–µ–π
        final_prompt = f"""
        –ù–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–∏—Ö –∞–Ω–∞–ª–∏–∑–æ–≤ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —á–∞—Å—Ç–µ–π –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤, —Å—Ñ–æ—Ä–º–∏—Ä—É–π –µ–¥–∏–Ω—É—é —Å–≤–æ–¥–∫—É –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å—É—Ç–∫–∏ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {category}.
        
        –ê–Ω–∞–ª–∏–∑—ã —á–∞—Å—Ç–µ–π:
        {chunk_analyses}
        
        –°—Ñ–æ—Ä–º–∏—Ä—É–π –æ—Ç–≤–µ—Ç –≤ —Å–ª–µ–¥—É—é—â–µ–º —Ñ–æ—Ä–º–∞—Ç–µ:

        üìÜ –°–≤–æ–¥–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π ‚Äî {category} ({analysis_start_date} - {end_date})

        üéÆ –ì–ª–∞–≤–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è:
        [–î–ª—è –∫–∞–∂–¥–æ–π –≥–ª–∞–≤–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏]
        - üìå –ó–∞–≥–æ–ª–æ–≤–æ–∫
        - üìù –û–ø–∏—Å–∞–Ω–∏–µ
        - üí° –í–ª–∏—è–Ω–∏–µ –Ω–∞ –∏–Ω–¥—É—Å—Ç—Ä–∏—é
        - üë• –†–µ–∞–∫—Ü–∏—è —Å–æ–æ–±—â–µ—Å—Ç–≤–∞
        - üîó –°—Å—ã–ª–∫–∞ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫

        üìä –û–±—â–∏–µ —Ç—Ä–µ–Ω–¥—ã:
        [–°–ø–∏—Å–æ–∫ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ç—Ä–µ–Ω–¥–æ–≤, –≤—ã—è–≤–ª–µ–Ω–Ω—ã—Ö –∑–∞ —Å—É—Ç–∫–∏]

        üß† –ê–Ω–∞–ª–∏–∑:
        [–û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ —Å–∏—Ç—É–∞—Ü–∏–∏, –≤–∫–ª—é—á–∞—è:
        - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
        - –ò–Ω–¥–µ–∫—Å —Ü–∏—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏
        - –ù–∞–∏–±–æ–ª–µ–µ –∞–∫—Ç–∏–≤–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        - –ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–≥–æ –º–µ–Ω–µ–¥–∂–µ—Ä–∞]

        üîÆ –ü—Ä–æ–≥–Ω–æ–∑:
        [–ö—Ä–∞—Ç–∫–∏–π –ø—Ä–æ–≥–Ω–æ–∑ —Ä–∞–∑–≤–∏—Ç–∏—è —Å–∏—Ç—É–∞—Ü–∏–∏]

        –ù–∞ –∫–∞–∂–¥—ã–π —Ä–∞–∑–¥–µ–ª –æ—Ç–≤–µ—á–∞–π –ø–æ–¥—Ä–æ–±–Ω–æ –∫–∞–∫ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫.
        """
        
        final_analysis = llm_client.analyze_text(
            prompt=final_prompt,
            query="\n".join(chunk_analyses)
        )
        
        return {
            'status': 'success',
            'analysis': final_analysis.get('analysis', ''),
            'materials_count': len(recent_materials),
            'chunks_count': len(chunks) if total_tokens > max_context_size * 0.8 else 1
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Ç—Ä–µ–Ω–¥–∞: {str(e)}")
        return {
            'status': 'error',
            'message': str(e)
        }

if __name__ == "__main__":
    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    embedding_size = test_embeddings()
    if embedding_size:
        logger.info(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {embedding_size}")
        if embedding_size != 1024:
            logger.warning(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ ({embedding_size}) –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç –æ–∂–∏–¥–∞–µ–º–æ–π (1024)")
            logger.warning("–ù–µ–æ–±—Ö–æ–¥–∏–º–æ –æ–±–Ω–æ–≤–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤ VectorStore –∏ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é")
    
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    result = analyze_trend(ANALYSIS_CATEGORY, ANALYSIS_START_DATE)
    
    if result['status'] == 'success':
        print("\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞:")
        print(f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤: {result['materials_count']}")
        print("\n–°–≤–æ–¥–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π:")
        print(result['analysis'])
    else:
        print(f"–û—à–∏–±–∫–∞: {result['message']}") 